{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) and was written by Erwan Lecarpentier and Jonathan Sprauel.\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">XGBoost<br>Introduction to XGBoost</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour :\n",
    "* In the **first notebook**, you learned the **basic of XGBoost**, how to apply it on a dataset and tune it to obtain the best performances.\n",
    "* In the **second notebook**, we will focus on **ensemble methods** and explain what makes XGBoost different from other models.\n",
    "* Finally in the **last notebook** you will see how the choice of a method (such as XGBoost) is a key element of a tradeoff between **Bias and Variance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# About Ensemble methods\n",
    "\n",
    "Ensemble method are based on the hypothesis that combining multiple models together can often produce a much more powerful model.\n",
    "\n",
    "The goal of this notebook is to understand and manipulate the model behind XGBoost, to better undertand the various parameters related to this model.\n",
    "We will have 4 exercices :\n",
    "* A short exercice focused on Weak Learners\n",
    "* Then we will see the difference between Boosting vs Bagging vs stacking\n",
    "* A specific focus on Ensemble Trees\n",
    "* Finally, you will learn to tune the parameters of XGBoost that are specific to trees.\n",
    "\n",
    "# Weak learners\n",
    "\n",
    "**Weak learners** are basics models that do not perform so well by themselves, either because they have a high bias (low degree of freedom models, for example) or because they have too much variance to be robust (high degree of freedom models, for example). The idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a strong learner (or ensemble model) that achieves better performances.\n",
    "\n",
    "In the following exemple, we will use a \"decision stumps.\" A decision stump is simply a decision tree where the whole tree is just one node. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 1</b><br>\n",
    "      Based on the following code, produce a weak learner on the given dataset.\n",
    "      <br>\n",
    "      Subsidiary question : what can be parallelized in the following code ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "class Stump:\n",
    "    def __init__(self):\n",
    "        self.gtLabel = None\n",
    "        self.ltLabel = None\n",
    "        self.splitThreshold = None\n",
    "        self.splitFeature = None\n",
    "\n",
    "    def predict(self, listpoint):\n",
    "        return np.array([self.predict_single(x) for x in listpoint])\n",
    "        \n",
    "    def predict_single(self, point):\n",
    "        if point[self.splitFeature] >= self.splitThreshold:\n",
    "            return self.gtLabel\n",
    "        else:\n",
    "            return self.ltLabel\n",
    "\n",
    "    def __call__(self, point):\n",
    "        return self.predict(point)\n",
    "\n",
    "    def errorFunction(self, data,index, t):\n",
    "        return sum([1.0 for x,y in data if self.makeThreshold(x,t,index) != y]) / len(data)\n",
    "\n",
    "\n",
    "    def makeThreshold(self,x, t,index):\n",
    "        return  1 if x[index] <= t else 0\n",
    "\n",
    "    def bestThreshold(self,data, index):\n",
    "        '''Compute best threshold for a given feature. Returns (threshold, error)'''\n",
    "\n",
    "        thresholds = [x[index] for x,y in data]\n",
    "        errors = [(threshold, self.errorFunction(data, index, threshold)) for threshold in thresholds]\n",
    "        return min(errors, key=lambda p: p[1])\n",
    "\n",
    "\n",
    "    def majorityVote(self,labels):\n",
    "        try:\n",
    "            return max(set(labels), key=labels.count)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def fit(self,X, Y):\n",
    "        data = list(zip(X,Y))\n",
    "        # find the index of the best feature to split on, and the best threshold for that index.\n",
    "\n",
    "        bestThresholds = [(i,) + self.bestThreshold(data, i) for i in range(len(X[0]))]\n",
    "        feature, thresh, _ = min(bestThresholds, key = lambda p: p[2])\n",
    "\n",
    "        self.splitFeature = feature\n",
    "        self.splitThreshold = thresh\n",
    "        self.gtLabel = self.majorityVote([y for x,y in data if x[self.splitFeature] >= self.splitThreshold])\n",
    "        self.ltLabel = self.majorityVote([y for x,y in data if x[self.splitFeature] < self.splitThreshold])\n",
    "\n",
    "\n",
    "\n",
    "breast = datasets.load_breast_cancer()\n",
    "#print(breast.DESCR)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(breast.data, breast.target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911185421524\n",
      "0.875946969697\n"
     ]
    }
   ],
   "source": [
    "stump = Stump()\n",
    "stump.fit(X_train,y_train)\n",
    "print(precision_score(y_train, stump.predict(X_train), average='macro'))\n",
    "print(precision_score(y_test, stump.predict(X_test), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885042870753\n",
      "The binary tree structure has 3 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 7] <= 0.0512799993157 else to node 2.\n",
      "\tnode=1 leaf node.\n",
      "\tnode=2 leaf node.\n"
     ]
    }
   ],
   "source": [
    "# you can use this code as a comparison point\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "tree = DecisionTreeClassifier(max_depth=1, random_state=0)\n",
    "tree.fit(X_train,y_train)\n",
    "print(precision_score(y_test, tree.predict(X_test), average='macro'))\n",
    "\n",
    "\n",
    "n_nodes = tree.tree_.node_count\n",
    "children_left = tree.tree_.children_left\n",
    "children_right = tree.tree_.children_right\n",
    "feature = tree.tree_.feature\n",
    "threshold = tree.tree_.threshold\n",
    "\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Boosting vs Bagging vs stacking\n",
    "The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "* **Bagging** considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "* **Boosting** considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy\n",
    "* **Stacking** considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Question 2</b><br>\n",
    "      Which kind is XGBoost ? Which kind is Random Forest ?\n",
    "</div>\n",
    "\n",
    "Each of these technics aims at improving either the *bias* or the *variance* of the individual weak learners.\n",
    "<img src=\"img/1 5pA6iY-qDP2JIsLoyfje-Q@2x.png\">\n",
    "\n",
    "## Focus on Boosting\n",
    "The intuition behind boosting is that each new model focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias. Boosting is a technic that is used with success by the the [top solutions](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927) in Kaggle competition - it is a crucial competence to master by going beyond using black-box library.\n",
    "\n",
    "Being mainly focused at reducing bias, the base models that are often considered for boosting are models with low variance but high bias. For instance, if we want to use trees as our base models, we will choose most of the time shallow decision trees with only a few depths.\n",
    "\n",
    "There are two emblematic Boosting algorithms : **Adaboost** and **Gradient Boosting**. Since Gradient Boosting is the algorithm used in XGboost, we will try to understand how it works.\n",
    "\n",
    "Here are the steps :\n",
    "* 1 - Fit a simple  decision tree on data\n",
    "* 2 - Calculate error residuals. Actual target value, minus predicted target value\n",
    "* 3 - Fit a new model on error residuals as target variable with same input variables\n",
    "* 4 - Add the predicted residuals to the previous predictions\n",
    "* 5 - Fit another model on residuals that is still left. Repeat steps 2 to 5 until overfitting or the sum of residuals becomes constant\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 3</b><br>\n",
    "      Use the following code to improve the weak learner by boosting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "# Ensemble Tree\n",
    "\n",
    "\n",
    "# Application on Regression\n",
    "Tree specific parameters (max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n",
    "subsample: percentage of samples used per tree. Low value can lead to underfitting.\n",
    "colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n",
    "n_estimators: number of trees you want to build.\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "https://jeremykun.com/2015/05/18/boosting-census/\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Takeway Questions**<br>\n",
    "a) What model is the model behind boosted trees?\n",
    "<br>\n",
    "b) What is the difference between a random forest and boosted trees?\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1\" data-toggle=\"collapse\">**Answers (click to unhide):**</a><br>\n",
    "<div id=\"answers1\" class=\"collapse\">\n",
    "a) Boosted trees are ensemble trees, i.e. a finite set of classification and regression trees. Like other ensemble methods, each tree participates in the prediction made by the model which is a sum of the outputs of all the trees.\n",
    "<br>\n",
    "b) Random forest and boosted trees use the same model but are trained in a different way. Random forests are trained using the technique of bootstrap aggregating, or bagging, while boosted trees are trained with gradient descent.\n",
    "<br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le modèle : gradient tree boosting\n",
    "\n",
    "visualisation du fichier\n",
    "visualisation de l'arbre\n",
    "\n",
    "bagging vs boosting\n",
    "\n",
    "difference entre xgboost et Random Forest\n",
    "* exercice sur la différence entre les deux\n",
    "\n",
    "les paramètres : l'impact de chaque paramètre visualisé\n",
    "\n",
    "* Exercice sur le fine tuning\n",
    "learning_rate: 0.01\n",
    "n_estimators: 100 if the size of your data is high, 1000 is if it is medium-low\n",
    "max_depth: 3\n",
    "subsample: 0.8\n",
    "colsample_bytree: 1\n",
    "gamma: 1\n",
    "\n",
    "https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster[0]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.426035523\n",
      "\t2:leaf=-0.218845025\n",
      "booster[1]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.213017777\n",
      "\t2:[f3<1.75] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.409090936\n",
      "\t\t\t6:leaf=-9.7534878e-009\n",
      "\t\t4:[f2<4.85000038] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-7.66345476e-009\n",
      "\t\t\t8:leaf=-0.210218996\n",
      "booster[2]:\n",
      "0:[f2<4.75] yes=1,no=2,missing=1\n",
      "\t1:[f3<1.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.217894763\n",
      "\t\t4:[f0<5.75] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-7.66345476e-009\n",
      "\t\t\t8:leaf=-0.155172437\n",
      "\t2:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t5:[f2<5.05000019] yes=9,no=10,missing=9\n",
      "\t\t\t9:leaf=-0.0360000096\n",
      "\t\t\t10:leaf=0.179999992\n",
      "\t\t6:[f2<4.85000038] yes=11,no=12,missing=11\n",
      "\t\t\t11:leaf=0.128571421\n",
      "\t\t\t12:leaf=0.420437962\n",
      "booster[3]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.293278694\n",
      "\t2:leaf=-0.195823714\n",
      "booster[4]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.189503655\n",
      "\t2:[f3<1.75] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.283187449\n",
      "\t\t\t6:leaf=-0.00450409995\n",
      "\t\t4:[f2<4.85000038] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.002580381\n",
      "\t\t\t8:leaf=-0.187011003\n",
      "booster[5]:\n",
      "0:[f2<4.75] yes=1,no=2,missing=1\n",
      "\t1:[f3<1.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.194471359\n",
      "\t\t4:[f0<5.75] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.012363052\n",
      "\t\t\t8:leaf=-0.136473104\n",
      "\t2:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t5:[f2<5.05000019] yes=9,no=10,missing=9\n",
      "\t\t\t9:leaf=-0.0313551426\n",
      "\t\t\t10:leaf=0.14433381\n",
      "\t\t6:leaf=0.282005012\n",
      "booster[6]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.234919116\n",
      "\t2:leaf=-0.180450663\n",
      "booster[7]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.173278794\n",
      "\t2:[f3<1.6500001] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.239570916\n",
      "\t\t\t6:leaf=-0.0643951893\n",
      "\t\t4:[f2<5.05000019] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.0446037501\n",
      "\t\t\t8:leaf=-0.168829933\n",
      "booster[8]:\n",
      "0:[f2<4.75] yes=1,no=2,missing=1\n",
      "\t1:[f3<1.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.178677022\n",
      "\t\t4:[f0<5.75] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.023564674\n",
      "\t\t\t8:leaf=-0.121302344\n",
      "\t2:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t5:[f2<5.05000019] yes=9,no=10,missing=9\n",
      "\t\t\t9:leaf=-0.0269564986\n",
      "\t\t\t10:leaf=0.118600301\n",
      "\t\t6:[f2<4.85000038] yes=11,no=12,missing=11\n",
      "\t\t\t11:leaf=0.0686386451\n",
      "\t\t\t12:leaf=0.233780459\n",
      "booster[9]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.202431291\n",
      "\t2:leaf=-0.16951783\n",
      "booster[10]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.16110006\n",
      "\t2:[f3<1.6500001] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.206351176\n",
      "\t\t\t6:leaf=-0.0582645871\n",
      "\t\t4:[f2<5.05000019] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.0372533724\n",
      "\t\t\t8:leaf=-0.156136811\n",
      "booster[11]:\n",
      "0:[f2<4.75] yes=1,no=2,missing=1\n",
      "\t1:[f3<1.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.167062446\n",
      "\t\t4:[f0<5.75] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.0278605875\n",
      "\t\t\t8:leaf=-0.107855745\n",
      "\t2:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t5:[f3<1.54999995] yes=9,no=10,missing=9\n",
      "\t\t\t9:leaf=0.107892469\n",
      "\t\t\t10:leaf=-0.0486981124\n",
      "\t\t6:[f2<4.85000038] yes=11,no=12,missing=11\n",
      "\t\t\t11:leaf=0.0537286513\n",
      "\t\t\t12:leaf=0.201581135\n",
      "booster[12]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.181833968\n",
      "\t2:leaf=-0.161279336\n",
      "booster[13]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.151241183\n",
      "\t2:[f3<1.6500001] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.18583031\n",
      "\t\t\t6:leaf=-0.0541694872\n",
      "\t\t4:[f2<5.05000019] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.0308365263\n",
      "\t\t\t8:leaf=-0.145577163\n",
      "booster[14]:\n",
      "0:[f2<4.75] yes=1,no=2,missing=1\n",
      "\t1:[f1<2.54999995] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.20000005] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.105130896\n",
      "\t\t\t8:leaf=0.0494744517\n",
      "\t\t4:leaf=-0.15946281\n",
      "\t2:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t5:[f3<1.54999995] yes=9,no=10,missing=9\n",
      "\t\t\t9:leaf=0.0952381566\n",
      "\t\t\t10:leaf=-0.0526574105\n",
      "\t\t6:[f2<4.85000038] yes=11,no=12,missing=11\n",
      "\t\t\t11:leaf=0.0416517556\n",
      "\t\t\t12:leaf=0.180929631\n",
      "booster[15]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.167452037\n",
      "\t2:leaf=-0.154637545\n",
      "booster[16]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.142646179\n",
      "\t2:[f3<1.6500001] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.171370044\n",
      "\t\t\t6:leaf=-0.0502402373\n",
      "\t\t4:[f2<5.05000019] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.0250191186\n",
      "\t\t\t8:leaf=-0.136175424\n",
      "booster[17]:\n",
      "0:[f2<4.75] yes=1,no=2,missing=1\n",
      "\t1:[f3<1.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.150957882\n",
      "\t\t4:leaf=-0.0241000354\n",
      "\t2:[f2<5.14999962] yes=5,no=6,missing=5\n",
      "\t\t5:[f1<2.9000001] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.107399516\n",
      "\t\t\t8:leaf=-0.0567395128\n",
      "\t\t6:leaf=0.171310365\n",
      "booster[18]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.156509727\n",
      "\t2:leaf=-0.148837656\n",
      "booster[19]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.13464351\n",
      "\t2:[f3<1.6500001] yes=3,no=4,missing=3\n",
      "\t\t3:[f2<4.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.160560519\n",
      "\t\t\t6:leaf=-0.0389119871\n",
      "\t\t4:[f1<2.95000005] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.147682503\n",
      "\t\t\t8:leaf=-0.0287340004\n",
      "booster[20]:\n",
      "0:[f3<1.6500001] yes=1,no=2,missing=1\n",
      "\t1:[f2<4.94999981] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.149492115\n",
      "\t\t4:leaf=0.0875303447\n",
      "\t2:[f1<3.1500001] yes=5,no=6,missing=5\n",
      "\t\t5:[f0<6.5999999] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.17925404\n",
      "\t\t\t8:leaf=0.0701403394\n",
      "\t\t6:[f0<6.35000038] yes=9,no=10,missing=9\n",
      "\t\t\t9:leaf=-0.0231665224\n",
      "\t\t\t10:leaf=0.108441807\n",
      "booster[21]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.147421569\n",
      "\t2:leaf=-0.143512979\n",
      "booster[22]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.126825705\n",
      "\t2:[f2<5.14999962] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.6500001] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.13780272\n",
      "\t\t\t6:leaf=-0.02103604\n",
      "\t\t4:leaf=-0.131201163\n",
      "booster[23]:\n",
      "0:[f3<1.6500001] yes=1,no=2,missing=1\n",
      "\t1:[f2<4.94999981] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.143016428\n",
      "\t\t4:leaf=0.0730580315\n",
      "\t2:[f2<5.05000019] yes=5,no=6,missing=5\n",
      "\t\t5:[f1<2.9000001] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.15045695\n",
      "\t\t\t8:leaf=-0.0804231763\n",
      "\t\t6:leaf=0.144651279\n",
      "booster[24]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.139379472\n",
      "\t2:leaf=-0.1382664\n",
      "booster[25]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.118962847\n",
      "\t2:[f2<5.14999962] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.6500001] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.12547642\n",
      "\t\t\t6:leaf=-0.015391604\n",
      "\t\t4:leaf=-0.122996815\n",
      "booster[26]:\n",
      "0:[f3<1.6500001] yes=1,no=2,missing=1\n",
      "\t1:[f2<4.94999981] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.136890367\n",
      "\t\t4:leaf=0.065978691\n",
      "\t2:[f2<5.05000019] yes=5,no=6,missing=5\n",
      "\t\t5:[f1<2.9000001] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.139393836\n",
      "\t\t\t8:leaf=-0.0777102485\n",
      "\t\t6:leaf=0.135768473\n",
      "booster[27]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.131838813\n",
      "\t2:leaf=-0.133041799\n",
      "booster[28]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f2<2.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.110990673\n",
      "\t\t4:[f3<1.6500001] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.113530107\n",
      "\t\t\t6:leaf=-0.0109445024\n",
      "\t2:leaf=-0.115016408\n",
      "booster[29]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.123578653\n",
      "\t2:[f2<5.14999962] yes=3,no=4,missing=3\n",
      "\t\t3:[f1<2.8499999] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0850512162\n",
      "\t\t\t6:leaf=-0.0855701342\n",
      "\t\t4:leaf=0.134624302\n",
      "booster[30]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.124599405\n",
      "\t2:leaf=-0.127767727\n",
      "booster[31]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f2<2.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.102955505\n",
      "\t\t4:[f1<2.54999995] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=-0.0234347507\n",
      "\t\t\t6:leaf=0.100213282\n",
      "\t2:leaf=-0.106150202\n",
      "booster[32]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.116745532\n",
      "\t2:[f2<5.14999962] yes=3,no=4,missing=3\n",
      "\t\t3:[f1<2.8499999] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0795386508\n",
      "\t\t\t6:leaf=-0.0784218684\n",
      "\t\t4:leaf=0.125638604\n",
      "booster[33]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.117440104\n",
      "\t2:leaf=-0.122576833\n",
      "booster[34]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f2<2.45000005] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.0949893519\n",
      "\t\t4:[f3<1.6500001] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0979773328\n",
      "\t\t\t6:leaf=-0.0199714694\n",
      "\t2:leaf=-0.097638227\n",
      "booster[35]:\n",
      "0:[f3<1.6500001] yes=1,no=2,missing=1\n",
      "\t1:[f2<4.94999981] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.120637245\n",
      "\t\t4:leaf=0.04401768\n",
      "\t2:[f1<3.1500001] yes=5,no=6,missing=5\n",
      "\t\t5:[f0<6.5999999] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.146946833\n",
      "\t\t\t8:leaf=0.0053314995\n",
      "\t\t6:leaf=-0.00888885651\n",
      "booster[36]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.110174023\n",
      "\t2:leaf=-0.11727383\n",
      "booster[37]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f0<5.44999981] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.0706041455\n",
      "\t\t4:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0943683237\n",
      "\t\t\t6:leaf=-0.0244200695\n",
      "\t2:leaf=-0.092852734\n",
      "booster[38]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.103084788\n",
      "\t2:[f2<5.14999962] yes=3,no=4,missing=3\n",
      "\t\t3:[f1<2.8499999] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0695324466\n",
      "\t\t\t6:leaf=-0.0695418492\n",
      "\t\t4:leaf=0.112138346\n",
      "booster[39]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.103750408\n",
      "\t2:leaf=-0.111847736\n",
      "booster[40]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f0<5.35000038] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.068919763\n",
      "\t\t4:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0817665085\n",
      "\t\t\t6:leaf=-0.0236760098\n",
      "\t2:leaf=-0.0850808471\n",
      "booster[41]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0965494439\n",
      "\t2:[f2<5.14999962] yes=3,no=4,missing=3\n",
      "\t\t3:[f1<2.5999999] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.102477513\n",
      "\t\t\t6:leaf=-0.0349612013\n",
      "\t\t4:leaf=0.104223654\n",
      "booster[42]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.0977719948\n",
      "\t2:leaf=-0.106630258\n",
      "booster[43]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f0<5.35000038] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.0617495328\n",
      "\t\t4:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0720256194\n",
      "\t\t\t6:leaf=-0.0240890998\n",
      "\t2:leaf=-0.0778691918\n",
      "booster[44]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0903428048\n",
      "\t2:[f1<3.1500001] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.00464527588\n",
      "\t\t\t6:leaf=0.123118788\n",
      "\t\t4:leaf=-0.0420078747\n",
      "booster[45]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.0921015888\n",
      "\t2:leaf=-0.101502113\n",
      "booster[46]:\n",
      "0:[f0<5.35000038] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0571368225\n",
      "\t2:[f3<1.75] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.54999995] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=-0.00546272285\n",
      "\t\t\t6:leaf=0.127619192\n",
      "\t\t4:[f2<4.85000038] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.0211822484\n",
      "\t\t\t8:leaf=-0.083902657\n",
      "booster[47]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0845012888\n",
      "\t2:[f1<3.04999995] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0116559332\n",
      "\t\t\t6:leaf=0.114607252\n",
      "\t\t4:leaf=-0.0383235216\n",
      "booster[48]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.086361222\n",
      "\t2:leaf=-0.0972094983\n",
      "booster[49]:\n",
      "0:[f0<5.35000038] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0528853573\n",
      "\t2:[f2<4.94999981] yes=3,no=4,missing=3\n",
      "\t\t3:[f0<5.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0966995507\n",
      "\t\t\t6:leaf=0.00185326627\n",
      "\t\t4:[f3<1.54999995] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.114780731\n",
      "\t\t\t8:leaf=0.0356381014\n",
      "booster[50]:\n",
      "0:[f2<4.44999981] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0800046176\n",
      "\t2:[f1<3.04999995] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0131886983\n",
      "\t\t\t6:leaf=0.108362883\n",
      "\t\t4:leaf=-0.038975127\n",
      "booster[51]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.081533812\n",
      "\t2:leaf=-0.0927178562\n",
      "booster[52]:\n",
      "0:[f0<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0511589833\n",
      "\t2:[f2<4.94999981] yes=3,no=4,missing=3\n",
      "\t\t3:[f0<5.94999981] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=0.0845774934\n",
      "\t\t\t6:leaf=0.00350863789\n",
      "\t\t4:[f0<6.05000019] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.00994144473\n",
      "\t\t\t8:leaf=-0.0478583612\n",
      "booster[53]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f1<3.04999995] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=-0.0264702719\n",
      "\t\t\t6:leaf=0.0966150165\n",
      "\t\t4:leaf=-0.112440191\n",
      "\t2:leaf=0.0878668576\n",
      "booster[54]:\n",
      "0:[f2<2.45000005] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.0769671798\n",
      "\t2:leaf=-0.0879491121\n",
      "booster[55]:\n",
      "0:[f1<2.6500001] yes=1,no=2,missing=1\n",
      "\t1:leaf=-0.0407986268\n",
      "\t2:[f3<1.75] yes=3,no=4,missing=3\n",
      "\t\t3:[f3<1.54999995] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=-0.0175688397\n",
      "\t\t\t6:leaf=0.116150878\n",
      "\t\t4:leaf=-0.030460164\n",
      "booster[56]:\n",
      "0:[f2<5.14999962] yes=1,no=2,missing=1\n",
      "\t1:[f1<2.54999995] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=0.0622623377\n",
      "\t\t4:[f3<1.75] yes=5,no=6,missing=5\n",
      "\t\t\t5:leaf=-0.0924694389\n",
      "\t\t\t6:leaf=0.017732814\n",
      "\t2:leaf=0.0820189714\n",
      "booster[57]:\n",
      "0:[f2<3.5999999] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.059880536\n",
      "\t2:leaf=-0.080451563\n",
      "booster[58]:\n",
      "0:[f2<4.94999981] yes=1,no=2,missing=1\n",
      "\t1:[f0<5.44999981] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.040299017\n",
      "\t\t4:[f0<5.94999981] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=0.0913687348\n",
      "\t\t\t8:leaf=0.00392305478\n",
      "\t2:[f0<6.05000019] yes=5,no=6,missing=5\n",
      "\t\t5:leaf=0.00590950903\n",
      "\t\t6:leaf=-0.0472565964\n",
      "booster[59]:\n",
      "0:[f2<4.94999981] yes=1,no=2,missing=1\n",
      "\t1:[f3<1.6500001] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=-0.0888596773\n",
      "\t\t4:leaf=0.0272979066\n",
      "\t2:[f3<1.54999995] yes=5,no=6,missing=5\n",
      "\t\t5:leaf=0.111694045\n",
      "\t\t6:[f3<1.75] yes=7,no=8,missing=7\n",
      "\t\t\t7:leaf=-0.0929203629\n",
      "\t\t\t8:leaf=0.0779465809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {'max_depth': 3,  'eta': 0.3, 'objective': 'multi:softprob', 'num_class': 3}\n",
    "num_round = 20 \n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "bst.dump_model('dump.raw.txt') # dump for model explanation\n",
    "\n",
    "with open('dump.raw.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of ypred1 = 0.033333\n",
      "error of ypred2 = 0.033333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ypred1 = bst.predict(dtest, ntree_limit=1)\n",
    "# by default, we predict using all the trees\n",
    "ypred2 = bst.predict(dtest)\n",
    "print('error of ypred1 = %f' % (np.sum((ypred1 > 0.5) != y_test) / float(len(y_test))))\n",
    "print('error of ypred2 = %f' % (np.sum((ypred2 > 0.5) != y_test) / float(len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sources :\n",
    "https://medium.com/@aravanshad/ensemble-methods-95533944783f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1**<br>\n",
    "\n",
    "c) In machine learning, models are trained with loss functions. What are the principal components of a loss function?\n",
    "<br>\n",
    "d) What is a regularization used for?\n",
    "</div>\n",
    "\n",
    "La visualisation de l'overfit avec la courbe d'apprentissage\n",
    "\n",
    "L'explication du trade of biais variance\n",
    "\n",
    "La feature importance\n",
    "\n",
    "La regularisation\n",
    "\n",
    "\n",
    "c) A loss function is composed of 1) a loss term, e.g. MSE, reflecting how far are the predictions of the model from the ground truth; 2) a regularization term, reflecting the complexity of the model.\n",
    "<br>\n",
    "d) A regularization term in a loss function is used to avoid over-fitting.\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "\n",
    "retour sur les paramètres avec Cross validation:\n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
